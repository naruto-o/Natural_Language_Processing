{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# simple text summary in python \n",
        "# import gensin module and summmarize function\n",
        "!pip install gensim\n",
        "from gensim.summarization.summarizer import summarize\n",
        "# paragraph\n",
        "paragraph=\"nnatural. language pro.cessing is t.he ability .of a compute.r. program to .understand. okay uncle how do. you do in. this wo.rld on. the cliff. jumping to thee river\"\n",
        "summ_per = summarize(paragraph,ratio=0.7)\n",
        "print(\"percent summary:\")\n",
        "print(summ_per)\n",
        "\n",
        "summ_words = summarize(paragraph,word_count=30)\n",
        "print(\"\\n\")\n",
        "print(\"word count summary\")\n",
        "print(summ_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s-RYDwNj0mn",
        "outputId": "2eb09cd4-1846-424f-8b25-3fcfec9cfe47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n",
            "WARNING:gensim.summarization.summarizer:Input text is expected to have at least 10 sentences.\n",
            "WARNING:gensim.summarization.summarizer:Input corpus is expected to have at least 10 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent summary:\n",
            "okay uncle how do.\n",
            "this wo.rld on.\n",
            "the cliff.\n",
            "jumping to thee river\n",
            "\n",
            "\n",
            "word count summary\n",
            "nnatural.\n",
            "language pro.cessing is t.he ability .of a compute.r. program to .understand.\n",
            "okay uncle how do.\n",
            "this wo.rld on.\n",
            "the cliff.\n",
            "jumping to thee river\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.summarization import keywords\n",
        "paragraph = \"nnatural. language pro.cessing is t.he ability .of a compute.r. program to .understand. okay uncle how do. you do in. this wo.rld on. the cliff. jumping to thee river\"\n",
        "keywords_txt = keywords(paragraph)\n",
        "print(keywords_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E86V9d6kojoT",
        "outputId": "dc3267fc-2560-4221-bb47-adfd912387e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "okay\n",
            "pro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "s1  = nlp(\"the weather is rainy\")\n",
        "s2 = nlp(\"it ii going to rain outside\")\n",
        "print(\"the similarity is:\",s1.similarity(s2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h61S1llojzL4",
        "outputId": "4cbbc117-2947-4a4a-c728-62f72aa00aa9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the similarity is: 0.1643198790345608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "words = [\"Machne\",\"Learnin\"]\n",
        "corrected_words = []\n",
        "for i in words:\n",
        "    corrected_words.append(TextBlob(i))\n",
        "print(\"wrong words :\",words)\n",
        "print(\"corrected words are :\")\n",
        "for  i in corrected_words:\n",
        "     print(i.correct(),end=\" \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTToF5SktMiZ",
        "outputId": "9702ff3e-0945-47f2-f43a-bace0c72142c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrong words : ['Machne', 'Learnin']\n",
            "corrected words are :\n",
            "Machine Learning "
          ]
        }
      ]
    }
  ]
}